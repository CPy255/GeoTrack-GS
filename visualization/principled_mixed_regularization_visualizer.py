#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸåˆ™æ€§æ··åˆæ­£åˆ™åŒ–å¯è§†åŒ–å™¨
Principled Mixed Regularization Visualizer

ç”¨äºå¯è§†åŒ–å‡ ä½•å…ˆéªŒå„å‘å¼‚æ€§æ­£åˆ™åŒ–çš„æ ¸å¿ƒæœºåˆ¶ï¼š
1. PCAå±€éƒ¨å‡ ä½•æ„ŸçŸ¥
2. ä¸‰é‡çº¦æŸæœºåˆ¶ï¼ˆä¸»è½´å¯¹é½ + å°ºåº¦æ¯”ä¾‹ + å„å‘å¼‚æ€§æƒ©ç½šï¼‰
3. æ··åˆæŸå¤±è®¾è®¡
4. æ­£åˆ™åŒ–å‰åæ•ˆæœå¯¹æ¯”

Author: AI Assistant
Date: 2025-01-25
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import Ellipse, FancyBboxPatch
import seaborn as sns
from scipy.spatial.distance import cdist
from sklearn.decomposition import PCA
# torchæ˜¯å¯é€‰çš„ï¼Œå¦‚æœä¸å­˜åœ¨ä¹Ÿä¸å½±å“å¯è§†åŒ–
try:
    import torch
except ImportError:
    torch = None
    
from pathlib import Path
import argparse
from typing import Tuple, List, Dict, Optional
import logging
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import LinearSegmentedColormap
from plyfile import PlyData, PlyElement
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from utils.geometry_regularization import GeometryRegularizer
    from scene.gaussian_model import GaussianModel
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥é¡¹ç›®æ¨¡å—ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
    GeometryRegularizer = None
    GaussianModel = None

# è®¾ç½®è‹±æ–‡å­—ä½“æ”¯æŒå’Œä¼˜åŒ–æ˜¾ç¤º
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans', 'Helvetica']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.size'] = 10
plt.rcParams['axes.titlesize'] = 12
plt.rcParams['axes.labelsize'] = 10
plt.rcParams['xtick.labelsize'] = 9
plt.rcParams['ytick.labelsize'] = 9
plt.rcParams['legend.fontsize'] = 9
plt.rcParams['figure.titlesize'] = 14

# è®¾ç½®ç§‘å­¦ç»˜å›¾é£æ ¼
sns.set_style("whitegrid")
try:
    plt.style.use('seaborn-v0_8-paper')
except OSError:
    try:
        plt.style.use('seaborn-v0_8-whitegrid')
    except OSError:
        # å¦‚æœæ—§ç‰ˆæœ¬æ ·å¼ä¹Ÿä¸å­˜åœ¨ï¼Œä½¿ç”¨æ–°ç‰ˆæœ¬
        try:
            plt.style.use('seaborn-paper')
        except OSError:
            plt.style.use('seaborn-whitegrid')


class PrincipledMixedRegularizationVisualizer:
    """Principled Mixed Regularization Visualizer"""
    
    def __init__(self, output_dir: str = "./regularization_visualization", 
                 model_path: Optional[str] = None):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            model_path: è®­ç»ƒæ¨¡å‹è·¯å¾„ï¼ˆåŒ…å«point_cloud.plyï¼‰
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.model_path = Path(model_path) if model_path else None
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # å¯è§†åŒ–å‚æ•°
        self.fig_size = (16, 12)  # é€‚ä¸­çš„é»˜è®¤å°ºå¯¸ 
        self.dpi = 200  # é™ä½DPIé¿å…æ–‡ä»¶è¿‡å¤§
        self.color_palette = sns.color_palette("husl", 8)
        
        # è‹±æ–‡æ˜¾ç¤ºä¼˜åŒ–
        plt.rcParams.update({
            'font.family': 'sans-serif',
            'font.size': 11,
            'axes.titlesize': 13,
            'figure.titlesize': 16
        })
        
        # åˆ›å»ºè‡ªå®šä¹‰é¢œè‰²æ˜ å°„
        self.gradient_cmap = LinearSegmentedColormap.from_list(
            "custom", ["#3498db", "#e74c3c"], N=256)
        
        self.logger.info(f"Principled Mixed Regularization Visualizer initialized")
        self.logger.info(f"Output directory: {self.output_dir}")
        if self.model_path:
            self.logger.info(f"Model path: {self.model_path}")
    
    def load_real_gaussian_data(self, ply_path: Optional[str] = None) -> Optional[Dict]:
        """
        åŠ è½½çœŸå®çš„é«˜æ–¯æ¨¡å‹æ•°æ®
        
        Args:
            ply_path: PLYæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä»model_pathè‡ªåŠ¨æ¨æ–­
            
        Returns:
            åŒ…å«é«˜æ–¯æ•°æ®çš„å­—å…¸ï¼Œå¦‚æœåŠ è½½å¤±è´¥è¿”å›None
        """
        if ply_path is None:
            if self.model_path is None:
                self.logger.warning("No model path provided, cannot load real data")
                return None
            
            # å°è¯•ä¸åŒå¯èƒ½çš„PLYæ–‡ä»¶è·¯å¾„
            possible_paths = [
                self.model_path / "point_cloud.ply",
                self.model_path / "point_cloud" / "iteration_30000" / "point_cloud.ply",
                self.model_path / "point_cloud" / "iteration_25000" / "point_cloud.ply", 
                self.model_path / "point_cloud" / "iteration_20000" / "point_cloud.ply",
                self.model_path / "point_cloud" / "iteration_15000" / "point_cloud.ply",
                self.model_path / "point_cloud" / "iteration_7000" / "point_cloud.ply",
            ]
            
            ply_path = None
            for path in possible_paths:
                if path.exists():
                    ply_path = path
                    break
            
            if ply_path is None:
                self.logger.warning(f"âŒ åœ¨ {self.model_path} ä¸­æœªæ‰¾åˆ°PLYæ–‡ä»¶")
                return None
        else:
            ply_path = Path(ply_path)
            if not ply_path.exists():
                self.logger.warning(f"âŒ PLYæ–‡ä»¶ä¸å­˜åœ¨: {ply_path}")
                return None
        
        self.logger.info(f"Loading real Gaussian data: {ply_path}")
        
        try:
            # è¯»å–PLYæ–‡ä»¶
            plydata = PlyData.read(str(ply_path))
            
            # æå–ä½ç½®
            xyz = np.stack((
                np.asarray(plydata.elements[0]["x"]),
                np.asarray(plydata.elements[0]["y"]),
                np.asarray(plydata.elements[0]["z"])
            ), axis=1)
            
            # æå–å°ºåº¦å‚æ•°
            scale_names = [p.name for p in plydata.elements[0].properties 
                          if p.name.startswith("scale_")]
            scale_names = sorted(scale_names, key=lambda x: int(x.split('_')[-1]))
            
            scales = np.zeros((xyz.shape[0], len(scale_names)))
            for idx, attr_name in enumerate(scale_names):
                scales[:, idx] = np.asarray(plydata.elements[0][attr_name])
            
            # æå–æ—‹è½¬å‚æ•°
            rot_names = [p.name for p in plydata.elements[0].properties 
                        if p.name.startswith("rot")]
            rot_names = sorted(rot_names, key=lambda x: int(x.split('_')[-1]))
            
            rotations = np.zeros((xyz.shape[0], len(rot_names)))
            for idx, attr_name in enumerate(rot_names):
                rotations[:, idx] = np.asarray(plydata.elements[0][attr_name])
            
            # æå–ä¸é€æ˜åº¦
            opacity = np.asarray(plydata.elements[0]["opacity"])
            
            # æ¿€æ´»å°ºåº¦å‚æ•°ï¼ˆä»logç©ºé—´è½¬æ¢ï¼‰
            scales_activated = np.exp(scales)
            
            n_gaussians = xyz.shape[0]
            self.logger.info(f"Successfully loaded {n_gaussians} Gaussian primitives")
            self.logger.info(f"ğŸ“Š ä½ç½®èŒƒå›´: X[{xyz[:, 0].min():.2f}, {xyz[:, 0].max():.2f}], "
                           f"Y[{xyz[:, 1].min():.2f}, {xyz[:, 1].max():.2f}], "
                           f"Z[{xyz[:, 2].min():.2f}, {xyz[:, 2].max():.2f}]")
            self.logger.info(f"ğŸ“ å°ºåº¦èŒƒå›´: {scales_activated.min():.4f} - {scales_activated.max():.4f}")
            
            return {
                'positions': xyz,
                'scales_before': scales_activated,  # è¿™æ˜¯å½“å‰çŠ¶æ€ï¼Œæˆ‘ä»¬ç”¨å®ƒä½œä¸º"ä¼˜åŒ–å‰"
                'rotations_before': rotations,
                'opacity': opacity,
                'n_gaussians': n_gaussians,
                'k_neighbors': min(16, max(8, n_gaussians // 10))  # è‡ªé€‚åº”Kå€¼
            }
            
        except Exception as e:
            self.logger.error(f"Failed to load PLY file: {e}")
            return None
    
    def generate_synthetic_gaussian_data(self, n_gaussians: int = 50, 
                                       k_neighbors: int = 8) -> Dict:
        """
        ç”Ÿæˆåˆæˆçš„é«˜æ–¯åŸºå…ƒæ•°æ®ç”¨äºå¯è§†åŒ–æ¼”ç¤º
        
        Args:
            n_gaussians: é«˜æ–¯åŸºå…ƒæ•°é‡
            k_neighbors: Kè¿‘é‚»æ•°é‡
            
        Returns:
            åŒ…å«é«˜æ–¯æ•°æ®çš„å­—å…¸
        """
        self.logger.info(f"ğŸ² ç”Ÿæˆ {n_gaussians} ä¸ªåˆæˆé«˜æ–¯åŸºå…ƒæ•°æ®...")
        
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
        np.random.seed(42)
        
        # ç”Ÿæˆ3Dä½ç½®ï¼ˆæ¨¡æ‹Ÿå¹³é¢å’Œè¾¹ç¼˜ç»“æ„ï¼‰
        positions = []
        
        # æ·»åŠ ä¸€äº›å¹³é¢ç»“æ„
        for i in range(n_gaussians // 3):
            # å¹³é¢1: XYå¹³é¢é™„è¿‘
            x = np.random.uniform(-5, 5)
            y = np.random.uniform(-5, 5) 
            z = np.random.normal(0, 0.5)
            positions.append([x, y, z])
        
        # æ·»åŠ ä¸€äº›è¾¹ç¼˜ç»“æ„
        for i in range(n_gaussians // 3):
            # è¾¹ç¼˜: Zè½´é™„è¿‘çš„çº¿æ€§ç»“æ„
            x = np.random.normal(0, 0.3)
            y = np.random.normal(0, 0.3)
            z = np.random.uniform(-3, 3)
            positions.append([x, y, z])
        
        # æ·»åŠ ä¸€äº›éšæœºç‚¹
        for i in range(n_gaussians - 2 * (n_gaussians // 3)):
            x = np.random.uniform(-6, 6)
            y = np.random.uniform(-6, 6)
            z = np.random.uniform(-3, 3)
            positions.append([x, y, z])
        
        positions = np.array(positions)
        
        # ç”Ÿæˆåˆå§‹é«˜æ–¯å‚æ•°ï¼ˆæœªæ­£åˆ™åŒ–ï¼‰
        scales_before = []
        rotations_before = []
        
        for i in range(n_gaussians):
            # ç”Ÿæˆéšæœºçš„ã€å¯èƒ½ä¸åˆç†çš„å°ºåº¦
            scale = np.random.exponential(1.0, 3)
            scale = np.sort(scale)[::-1]  # é™åºæ’åˆ—
            # æ·»åŠ ä¸€äº›è¿‡åº¦å„å‘å¼‚æ€§çš„æƒ…å†µ
            if np.random.random() < 0.3:
                scale[0] *= 5  # ä¸»è½´è¿‡åº¦æ‹‰ä¼¸
            scales_before.append(scale)
            
            # ç”Ÿæˆéšæœºæ—‹è½¬ï¼ˆå››å…ƒæ•°ï¼‰
            # ç®€åŒ–ä¸ºæ—‹è½¬è§’åº¦
            angles = np.random.uniform(0, 2*np.pi, 3)
            rotations_before.append(angles)
        
        scales_before = np.array(scales_before)
        rotations_before = np.array(rotations_before)
        
        return {
            'positions': positions,
            'scales_before': scales_before,
            'rotations_before': rotations_before,
            'k_neighbors': k_neighbors,
            'n_gaussians': n_gaussians
        }
    
    def compute_local_pca_analysis(self, positions: np.ndarray, 
                                 k_neighbors: int = 8) -> Dict:
        """
        è®¡ç®—å±€éƒ¨PCAåˆ†æï¼Œæ¨¡æ‹ŸGeometryRegularizerçš„è¡Œä¸º
        
        Args:
            positions: é«˜æ–¯ä½ç½® (N, 3)
            k_neighbors: Kè¿‘é‚»æ•°é‡
            
        Returns:
            PCAåˆ†æç»“æœ
        """
        self.logger.info(f"ğŸ” æ‰§è¡Œå±€éƒ¨PCAåˆ†æ (K={k_neighbors})...")
        
        n_points = len(positions)
        principal_directions = []
        eigenvalues_list = []
        neighbor_indices_list = []
        
        # è®¡ç®—è·ç¦»çŸ©é˜µï¼ˆå¯¹å¤§æ•°æ®é›†è¿›è¡Œä¼˜åŒ–ï¼‰
        if n_points > 5000:
            self.logger.warning(f"æ•°æ®é›†è¾ƒå¤§({n_points}ä¸ªç‚¹)ï¼ŒPCAåˆ†æå¯èƒ½è¾ƒæ…¢")
        distances = cdist(positions, positions)
        
        for i in range(n_points):
            # æ‰¾åˆ°Kä¸ªæœ€è¿‘é‚»ï¼ˆæ’é™¤è‡ªå·±ï¼‰
            neighbor_indices = np.argsort(distances[i])[1:k_neighbors+1]
            neighbor_indices_list.append(neighbor_indices)
            
            # è·å–é‚»å±…ä½ç½®
            neighbors = positions[neighbor_indices]
            
            # è®¡ç®—è´¨å¿ƒ
            centroid = neighbors.mean(axis=0)
            
            # ä¸­å¿ƒåŒ–
            centered_neighbors = neighbors - centroid
            
            # PCAåˆ†æ
            if len(centered_neighbors) > 1:
                pca = PCA(n_components=3)
                pca.fit(centered_neighbors)
                
                principal_directions.append(pca.components_)
                eigenvalues_list.append(pca.explained_variance_)
            else:
                # å¤„ç†é‚»å±…ä¸è¶³çš„æƒ…å†µ
                principal_directions.append(np.eye(3))
                eigenvalues_list.append(np.ones(3))
        
        principal_directions = np.array(principal_directions)
        eigenvalues_array = np.array(eigenvalues_list)
        
        return {
            'principal_directions': principal_directions,
            'eigenvalues': eigenvalues_array,
            'neighbor_indices': neighbor_indices_list
        }
    
    def simulate_regularization_effects(self, data: Dict, pca_results: Dict) -> Dict:
        """
        æ¨¡æ‹Ÿæ­£åˆ™åŒ–æ•ˆæœï¼Œç”Ÿæˆæ­£åˆ™åŒ–åçš„é«˜æ–¯å‚æ•°
        
        Args:
            data: åŸå§‹é«˜æ–¯æ•°æ®
            pca_results: PCAåˆ†æç»“æœ
            
        Returns:
            æ­£åˆ™åŒ–åçš„ç»“æœ
        """
        self.logger.info("âš™ï¸ æ¨¡æ‹Ÿæ­£åˆ™åŒ–æ•ˆæœ...")
        
        positions = data['positions']
        scales_before = data['scales_before']
        principal_directions = pca_results['principal_directions']
        eigenvalues = pca_results['eigenvalues']
        
        n_gaussians = len(positions)
        scales_after = []
        alignment_scores = []
        ratio_improvements = []
        anisotropy_penalties = []
        
        for i in range(n_gaussians):
            # è·å–å±€éƒ¨å‡ ä½•ä¿¡æ¯
            local_directions = principal_directions[i]
            local_eigenvals = eigenvalues[i]
            
            # æ¨¡æ‹Ÿä¸»è½´å¯¹é½çº¦æŸæ•ˆæœ
            original_main_axis = np.array([1, 0, 0])  # å‡è®¾åŸå§‹ä¸»è½´
            geometry_main_axis = local_directions[0]  # PCAä¸»æ–¹å‘
            
            # è®¡ç®—å¯¹é½åˆ†æ•°ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            alignment_score = np.abs(np.dot(original_main_axis, geometry_main_axis))
            alignment_scores.append(alignment_score)
            
            # æ¨¡æ‹Ÿå°ºåº¦æ¯”ä¾‹çº¦æŸæ•ˆæœ
            original_scale = scales_before[i]
            target_ratio = local_eigenvals[0] / (local_eigenvals[2] + 1e-8)
            
            # è°ƒæ•´å°ºåº¦ä½¿å…¶æ›´ç¬¦åˆå±€éƒ¨å‡ ä½•
            adjusted_scale = original_scale.copy()
            if target_ratio < 20:  # åˆç†çš„æ¯”ä¾‹èŒƒå›´
                adjusted_scale[0] = adjusted_scale[2] * min(target_ratio, 10)
            
            # æ¨¡æ‹Ÿè¿‡åº¦å„å‘å¼‚æ€§æƒ©ç½š
            current_ratio = adjusted_scale[0] / (adjusted_scale[2] + 1e-8)
            if current_ratio > 10:
                adjusted_scale[0] = adjusted_scale[2] * 8  # é™åˆ¶æœ€å¤§æ¯”ä¾‹
            
            anisotropy_penalty = max(0, current_ratio - 10)
            anisotropy_penalties.append(anisotropy_penalty)
            
            scales_after.append(adjusted_scale)
            
            # è®¡ç®—æ”¹å–„ç¨‹åº¦
            ratio_before = scales_before[i][0] / (scales_before[i][2] + 1e-8)
            ratio_after = adjusted_scale[0] / (adjusted_scale[2] + 1e-8)
            ratio_improvement = abs(ratio_before - target_ratio) - abs(ratio_after - target_ratio)
            ratio_improvements.append(max(0, ratio_improvement))
        
        scales_after = np.array(scales_after)
        
        return {
            'scales_after': scales_after,
            'alignment_scores': np.array(alignment_scores),
            'ratio_improvements': np.array(ratio_improvements),
            'anisotropy_penalties': np.array(anisotropy_penalties)
        }
    
    def create_mixed_regularization_visualization(self, use_real_data: bool = True):
        """åˆ›å»ºå®Œæ•´çš„åŸåˆ™æ€§æ··åˆæ­£åˆ™åŒ–å¯è§†åŒ–å›¾"""
        self.logger.info("ğŸ¨ åˆ›å»ºåŸåˆ™æ€§æ··åˆæ­£åˆ™åŒ–å¯è§†åŒ–å›¾...")
        
        # å°è¯•åŠ è½½çœŸå®æ•°æ®
        data = None
        if use_real_data and self.model_path:
            data = self.load_real_gaussian_data()
        
        # å¦‚æœçœŸå®æ•°æ®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åˆæˆæ•°æ®
        if data is None:
            self.logger.info("ğŸ“Š ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º...")
            data = self.generate_synthetic_gaussian_data(n_gaussians=30, k_neighbors=8)
        else:
            self.logger.info("ğŸ“Š ä½¿ç”¨çœŸå®è®­ç»ƒæ•°æ®è¿›è¡Œå¯è§†åŒ–...")
            # å¯¹äºå¤§å‹æ•°æ®é›†ï¼Œè¿›è¡Œé‡‡æ ·ä»¥æé«˜å¯è§†åŒ–æ€§èƒ½
            n_gaussians = data['n_gaussians']
            if n_gaussians > 1000:
                self.logger.info(f"ğŸ¯ å¤§æ•°æ®é›†é‡‡æ ·: {n_gaussians} -> 1000 ä¸ªé«˜æ–¯åŸºå…ƒ")
                indices = np.random.choice(n_gaussians, 1000, replace=False)
                data['positions'] = data['positions'][indices]
                data['scales_before'] = data['scales_before'][indices]
                data['rotations_before'] = data['rotations_before'][indices]
                data['n_gaussians'] = 1000
        
        pca_results = self.compute_local_pca_analysis(data['positions'], data['k_neighbors'])
        reg_results = self.simulate_regularization_effects(data, pca_results)
        
        # åˆ›å»ºä¸»å›¾ - ä½¿ç”¨åˆç†çš„å°ºå¯¸å’Œå¸ƒå±€
        fig = plt.figure(figsize=(18, 10))  # è°ƒæ•´ä¸ºæ›´åˆç†çš„å®½é«˜æ¯”
        
        # ä½¿ç”¨GridSpecæ¥æ›´å¥½åœ°æ§åˆ¶å­å›¾å¸ƒå±€ - ä¿®å¤å¸ƒå±€é—®é¢˜
        from matplotlib import gridspec
        gs = gridspec.GridSpec(2, 3, figure=fig, hspace=0.45, wspace=0.25)
        
        # è°ƒæ•´æ ‡é¢˜ä½ç½®é¿å…ä¸å­å›¾é‡å 
        fig.suptitle('Geometry-Prior Anisotropic Regularization: Principled Mixed Mechanism\n'
                    'Visualization of Mixed Regularization Effects', 
                    fontsize=16, fontweight='bold', y=0.96)
        
        # å­å›¾1: PCAå±€éƒ¨å‡ ä½•æ„ŸçŸ¥ (å·¦ä¸Š)
        try:
            ax1 = fig.add_subplot(gs[0, 0], projection='3d')
            self._plot_pca_local_geometry(ax1, data, pca_results)
        except Exception as e:
            self.logger.error(f"Error plotting PCA geometry: {e}")
        
        # å­å›¾2: ä¸‰é‡çº¦æŸæœºåˆ¶ (ä¸­ä¸Š)
        try:
            ax2 = fig.add_subplot(gs[0, 1])
            self._plot_triple_constraint_mechanism(ax2, data, reg_results)
        except Exception as e:
            self.logger.error(f"Error plotting constraint mechanism: {e}")
        
        # å­å›¾3: æ··åˆæŸå¤±æƒé‡ (å³ä¸Š)
        try:
            ax3 = fig.add_subplot(gs[0, 2])
            self._plot_mixed_loss_weights(ax3, reg_results)
        except Exception as e:
            self.logger.error(f"Error plotting loss weights: {e}")
        
        # å­å›¾4: æ­£åˆ™åŒ–å‰åå¯¹æ¯” (å·¦ä¸‹)
        try:
            ax4 = fig.add_subplot(gs[1, 0])
            self._plot_before_after_comparison(ax4, data, reg_results)
        except Exception as e:
            self.logger.error(f"Error plotting before/after comparison: {e}")
        
        # å­å›¾5: æ•ˆæœé‡åŒ–åˆ†æ (ä¸­ä¸‹)
        try:
            ax5 = fig.add_subplot(gs[1, 1])
            self._plot_quantitative_analysis(ax5, data, reg_results)
        except Exception as e:
            self.logger.error(f"Error plotting quantitative analysis: {e}")
        
        # å­å›¾6: åŸç†æµç¨‹å›¾ (å³ä¸‹)
        try:
            ax6 = fig.add_subplot(gs[1, 2])
            self._plot_principle_flowchart(ax6)
        except Exception as e:
            self.logger.error(f"Error plotting flowchart: {e}")
        

        plt.tight_layout(rect=[0, 0, 1, 0.94]) 

        # ä¿å­˜å›¾åƒ - æ·»åŠ é”™è¯¯å¤„ç†å’Œä¼˜åŒ–è®¾ç½®
        output_path = self.output_dir / 'principled_mixed_regularization.png'
        try:
          # ç§»é™¤ bbox_inches='tight'ï¼Œå› ä¸º tight_layout å·²ç»å®Œæˆäº†å¸ƒå±€å·¥ä½œ
            plt.savefig(output_path, dpi=200, 
                      facecolor='white', edgecolor='none', 
                      format='png', pil_kwargs={'optimize': True})
            self.logger.info(f"Successfully saved: {output_path}")
        except Exception as e:
            self.logger.error(f"Failed to save main figure: {e}")
            # å°è¯•ç”¨æ›´ç®€å•çš„è®¾ç½®ä¿å­˜
            try:
                plt.savefig(output_path, dpi=150, format='png')
                self.logger.info(f"Saved with fallback settings: {output_path}")
            except Exception as e2:
                self.logger.error(f"Fallback save also failed: {e2}")
        finally:
            plt.close()
        
        self.logger.info(f"âœ… åŸåˆ™æ€§æ··åˆæ­£åˆ™åŒ–å›¾å·²ä¿å­˜: {output_path}")
        
        # åˆ›å»ºè¯¦ç»†åˆ†æå›¾
        self._create_detailed_analysis_figures(data, pca_results, reg_results)
    
    def _plot_pca_local_geometry(self, ax, data: Dict, pca_results: Dict):
        """ç»˜åˆ¶PCAå±€éƒ¨å‡ ä½•æ„ŸçŸ¥"""
        positions = data['positions']
        principal_directions = pca_results['principal_directions']
        
        # é€‰æ‹©ä¸€ä¸ªç¤ºä¾‹ç‚¹è¿›è¡Œè¯¦ç»†å±•ç¤º
        example_idx = 10
        example_pos = positions[example_idx]
        neighbor_indices = pca_results['neighbor_indices'][example_idx]
        
        # ç»˜åˆ¶æ‰€æœ‰ç‚¹ï¼ˆå°é€æ˜ç‚¹ï¼‰
        ax.scatter(positions[:, 0], positions[:, 1], positions[:, 2], 
                  c='lightgray', alpha=0.3, s=20)
        
        # çªå‡ºæ˜¾ç¤ºç¤ºä¾‹ç‚¹
        ax.scatter(*example_pos, c='red', s=100, label='Center Point')
        
        # çªå‡ºæ˜¾ç¤ºé‚»å±…ç‚¹
        neighbors = positions[neighbor_indices]
        ax.scatter(neighbors[:, 0], neighbors[:, 1], neighbors[:, 2], 
                  c='blue', s=60, alpha=0.7, label=f'K={len(neighbor_indices)} Neighbors')
        
        # ç»˜åˆ¶è¿æ¥çº¿
        for neighbor in neighbors:
            ax.plot([example_pos[0], neighbor[0]], 
                   [example_pos[1], neighbor[1]], 
                   [example_pos[2], neighbor[2]], 
                   'b--', alpha=0.5, linewidth=0.8)
        
        # ç»˜åˆ¶PCAä¸»æ–¹å‘
        principal_dirs = principal_directions[example_idx]
        eigenvals = pca_results['eigenvalues'][example_idx]
        
        colors = ['red', 'green', 'orange']
        for i, (direction, eigenval) in enumerate(zip(principal_dirs, eigenvals)):
            scale = np.sqrt(eigenval) * 2
            end_point = example_pos + direction * scale
            ax.plot([example_pos[0], end_point[0]], 
                   [example_pos[1], end_point[1]], 
                   [example_pos[2], end_point[2]], 
                   color=colors[i], linewidth=3, alpha=0.8,
                   label=f'PC{i+1} (Î»={eigenval:.2f})')
        
        ax.set_title('Local Geometry Perception via PCA\nNeighborhood Analysis & Principal Components', fontweight='bold')
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_zlabel('Z')
        ax.legend(fontsize=8)
        
        # è®¾ç½®è§†è§’
        ax.view_init(elev=20, azim=45)
    
    def _plot_triple_constraint_mechanism(self, ax, data: Dict, reg_results: Dict):
        """ç»˜åˆ¶ä¸‰é‡çº¦æŸæœºåˆ¶"""
        alignment_scores = reg_results['alignment_scores']
        ratio_improvements = reg_results['ratio_improvements']
        anisotropy_penalties = reg_results['anisotropy_penalties']
        
        n_gaussians = len(alignment_scores)
        x = np.arange(n_gaussians)
        
        # åˆ›å»ºå †å æ¡å½¢å›¾å±•ç¤ºä¸‰ç§çº¦æŸçš„ç›¸å¯¹é‡è¦æ€§
        width = 0.8
        
        # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´ä»¥ä¾¿æ¯”è¾ƒ
        alignment_norm = alignment_scores / (alignment_scores.max() + 1e-8)
        ratio_norm = ratio_improvements / (ratio_improvements.max() + 1e-8)
        penalty_norm = anisotropy_penalties / (anisotropy_penalties.max() + 1e-8)
        
        # ç»˜åˆ¶å †å æ¡å½¢å›¾
        p1 = ax.bar(x, alignment_norm, width, label='Axis Alignment', 
                   color=self.color_palette[0], alpha=0.8)
        p2 = ax.bar(x, ratio_norm, width, bottom=alignment_norm, 
                   label='Scale Ratio', color=self.color_palette[1], alpha=0.8)
        p3 = ax.bar(x, penalty_norm, width, 
                   bottom=alignment_norm + ratio_norm,
                   label='Anisotropy Penalty', color=self.color_palette[2], alpha=0.8)
        
        ax.set_title('Triple Constraint Mechanism\nAlignment, Ratio & Anisotropy Penalties', fontweight='bold')
        ax.set_xlabel('Gaussian Index')
        ax.set_ylabel('Normalized Constraint Strength')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ æ–‡æœ¬è¯´æ˜
        ax.text(0.02, 0.98, 'â€¢ Axis Alignment: Align with geometry\nâ€¢ Scale Ratio: Match eigenvalue ratios\nâ€¢ Anisotropy: Prevent over-stretching', 
               transform=ax.transAxes, fontsize=9, verticalalignment='top',
               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
    
    def _plot_mixed_loss_weights(self, ax, reg_results: Dict):
        """ç»˜åˆ¶æ··åˆæŸå¤±æƒé‡"""
        # æ¨¡æ‹Ÿä¸åŒè®­ç»ƒé˜¶æ®µçš„æŸå¤±æƒé‡å˜åŒ–
        iterations = np.arange(0, 10000, 100)
        n_iter = len(iterations)
        
        # æ¨¡æ‹ŸæŸå¤±åˆ†é‡éšè®­ç»ƒå˜åŒ–
        alignment_loss = np.exp(-iterations / 3000) * 0.5 + 0.1
        ratio_loss = np.exp(-iterations / 5000) * 0.3 + 0.05  
        anisotropy_penalty = np.maximum(0, (iterations - 2000) / 8000) * 0.2
        
        # ç»˜åˆ¶æŸå¤±åˆ†é‡æ›²çº¿
        ax.plot(iterations, alignment_loss, label='Axis Alignment Loss', 
               color=self.color_palette[0], linewidth=2)
        ax.plot(iterations, ratio_loss, label='Scale Ratio Loss', 
               color=self.color_palette[1], linewidth=2)
        ax.plot(iterations, anisotropy_penalty, label='Anisotropy Penalty', 
               color=self.color_palette[2], linewidth=2)
        
        # æ€»æŸå¤±
        total_loss = alignment_loss + 0.5 * ratio_loss + 0.1 * anisotropy_penalty
        ax.plot(iterations, total_loss, label='Total Mixed Loss', 
               color='black', linewidth=3, linestyle='--')
        
        ax.set_title('Mixed Loss Weight Evolution\nTraining Progress Analysis', fontweight='bold')
        ax.set_xlabel('Training Iterations')
        ax.set_ylabel('Loss Value')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ å…¬å¼æ³¨é‡Š
        ax.text(0.02, 0.98, r'$L_{total} = L_{align} + 0.5 \cdot L_{ratio} + 0.1 \cdot L_{penalty}$', 
               transform=ax.transAxes, fontsize=10, verticalalignment='top',
               bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', alpha=0.8))
    
    def _plot_before_after_comparison(self, ax, data: Dict, reg_results: Dict):
        """ç»˜åˆ¶æ­£åˆ™åŒ–å‰åå¯¹æ¯”"""
        scales_before = data['scales_before']
        scales_after = reg_results['scales_after']
        
        # è®¡ç®—å°ºåº¦æ¯”ä¾‹ï¼ˆä¸»è½´/æ¬¡è½´ï¼‰
        ratios_before = scales_before[:, 0] / (scales_before[:, 2] + 1e-8)
        ratios_after = scales_after[:, 0] / (scales_after[:, 2] + 1e-8)
        
        # åˆ›å»ºæ•£ç‚¹å›¾å¯¹æ¯”
        n_gaussians = len(ratios_before)
        x = np.arange(n_gaussians)
        
        # ç»˜åˆ¶æ•£ç‚¹å›¾
        ax.scatter(x, ratios_before, c='red', alpha=0.7, s=50, 
                  label='Before Regularization', marker='o')
        ax.scatter(x, ratios_after, c='blue', alpha=0.7, s=50, 
                  label='After Regularization', marker='s')
        
        # è¿æ¥çº¿æ˜¾ç¤ºå˜åŒ–
        for i in range(n_gaussians):
            ax.plot([i, i], [ratios_before[i], ratios_after[i]], 
                   'gray', alpha=0.5, linewidth=1)
        
        # æ·»åŠ ç†æƒ³èŒƒå›´
        ax.axhline(y=10, color='green', linestyle='--', alpha=0.7, 
                  label='Ideal Upper Limit (10:1)')
        ax.fill_between(x, 1, 10, alpha=0.1, color='green', 
                       label='Ideal Range')
        
        ax.set_title('Before vs After Regularization\nAnisotropy Ratio Comparison', fontweight='bold')
        ax.set_xlabel('Gaussian Index')
        ax.set_ylabel('Anisotropy Ratio (Major/Minor Axis)')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_yscale('log')
    
    def _plot_quantitative_analysis(self, ax, data: Dict, reg_results: Dict):
        """ç»˜åˆ¶æ•ˆæœé‡åŒ–åˆ†æ"""
        # è®¡ç®—å„ç§æ”¹å–„æŒ‡æ ‡
        scales_before = data['scales_before']
        scales_after = reg_results['scales_after']
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            'Over-stretch\nReduction': self._calculate_over_stretch_reduction(scales_before, scales_after),
            'Geometry\nAlignment': np.mean(reg_results['alignment_scores']) * 100,
            'Scale Ratio\nImprovement': np.mean(reg_results['ratio_improvements']) * 100,
            'Shape\nStability': (1 - np.mean(reg_results['anisotropy_penalties'])) * 100
        }
        
        # åˆ›å»ºæ¡å½¢å›¾
        names = list(metrics.keys())
        values = list(metrics.values())
        colors = [self.color_palette[i] for i in range(len(names))]
        
        bars = ax.bar(names, values, color=colors, alpha=0.8)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                   f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        ax.set_title('Quantitative Effect Analysis\nImprovement Metrics', fontweight='bold')
        ax.set_ylabel('Improvement Percentage (%)')
        ax.set_ylim(0, 100)
        ax.grid(True, alpha=0.3, axis='y')
        
        # æ—‹è½¬xè½´æ ‡ç­¾ - ä½¿ç”¨wrapæ¥é¿å…æ ‡ç­¾é‡å 
        ax.tick_params(axis='x', labelsize=9)
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
    
    def _plot_principle_flowchart(self, ax):
        """ç»˜åˆ¶åŸç†æµç¨‹å›¾"""
        ax.set_xlim(0, 10)
        ax.set_ylim(0, 10)
        ax.axis('off')
        
        # æµç¨‹æ¡†
        boxes = [
            {'pos': (2, 8.5), 'text': 'Input\nGaussians', 'color': 'lightblue'},
            {'pos': (2, 7), 'text': 'K-NN\nSearch', 'color': 'lightgreen'},
            {'pos': (2, 5.5), 'text': 'PCA\nAnalysis', 'color': 'lightgreen'},
            {'pos': (2, 4), 'text': 'Local Geometry\nPrior', 'color': 'lightcoral'},
            {'pos': (6, 7.5), 'text': 'Axis Alignment\nConstraint', 'color': 'lightyellow'},
            {'pos': (6, 6), 'text': 'Scale Ratio\nConstraint', 'color': 'lightyellow'},
            {'pos': (6, 4.5), 'text': 'Anisotropy\nPenalty', 'color': 'lightyellow'},
            {'pos': (8.5, 6), 'text': 'Mixed Loss\nFunction', 'color': 'lightgray'},
            {'pos': (8.5, 3), 'text': 'Optimized\nGaussians', 'color': 'lightsteelblue'}
        ]
        
        # ç»˜åˆ¶æ¡†
        for box in boxes:
            rect = FancyBboxPatch(
                (box['pos'][0] - 0.7, box['pos'][1] - 0.4),
                1.4, 0.8,
                boxstyle="round,pad=0.1",
                facecolor=box['color'],
                edgecolor='black',
                linewidth=1
            )
            ax.add_patch(rect)
            ax.text(box['pos'][0], box['pos'][1], box['text'], 
                   ha='center', va='center', fontweight='bold', fontsize=9)
        
        # ç»˜åˆ¶ç®­å¤´
        arrows = [
            ((2, 8.1), (2, 7.4)),  # è¾“å…¥ -> Kè¿‘é‚»
            ((2, 6.6), (2, 5.9)),  # Kè¿‘é‚» -> PCA
            ((2, 5.1), (2, 4.4)),  # PCA -> å‡ ä½•å…ˆéªŒ
            ((2.7, 4), (5.3, 7.5)),  # å‡ ä½•å…ˆéªŒ -> ä¸»è½´å¯¹é½
            ((2.7, 4), (5.3, 6)),    # å‡ ä½•å…ˆéªŒ -> å°ºåº¦æ¯”ä¾‹
            ((2.7, 4), (5.3, 4.5)),  # å‡ ä½•å…ˆéªŒ -> å„å‘å¼‚æ€§
            ((6.7, 7.5), (7.8, 6.3)),  # ä¸»è½´å¯¹é½ -> æ··åˆæŸå¤±
            ((6.7, 6), (7.8, 6)),      # å°ºåº¦æ¯”ä¾‹ -> æ··åˆæŸå¤±
            ((6.7, 4.5), (7.8, 5.7)),  # å„å‘å¼‚æ€§ -> æ··åˆæŸå¤±
            ((8.5, 5.6), (8.5, 3.4))   # æ··åˆæŸå¤± -> è¾“å‡º
        ]
        
        for start, end in arrows:
            ax.annotate('', xy=end, xytext=start,
                       arrowprops=dict(arrowstyle='->', lw=1.5, color='darkblue'))
        
        ax.set_title('Principled Mixed Regularization Flow\nEnd-to-End Processing Pipeline', 
                    fontweight='bold', pad=20)
    
    def _calculate_over_stretch_reduction(self, scales_before: np.ndarray, 
                                        scales_after: np.ndarray, threshold: float = 10.0) -> float:
        """è®¡ç®—è¿‡åº¦æ‹‰ä¼¸å‡å°‘ç‡"""
        ratios_before = scales_before[:, 0] / (scales_before[:, 2] + 1e-8)
        ratios_after = scales_after[:, 0] / (scales_after[:, 2] + 1e-8)
        
        over_stretch_before = np.sum(ratios_before > threshold)
        over_stretch_after = np.sum(ratios_after > threshold)
        
        if over_stretch_before == 0:
            return 100.0
        
        reduction_rate = (over_stretch_before - over_stretch_after) / over_stretch_before * 100
        return max(0, reduction_rate)
    
    def _create_detailed_analysis_figures(self, data: Dict, pca_results: Dict, reg_results: Dict):
        """åˆ›å»ºè¯¦ç»†åˆ†æå›¾"""
        self.logger.info("ğŸ“ˆ åˆ›å»ºè¯¦ç»†åˆ†æå›¾...")
        
        # åˆ›å»ºPCAåˆ†æè¯¦ç»†å›¾
        self._create_pca_analysis_figure(data, pca_results)
        
        # åˆ›å»ºæŸå¤±åˆ†é‡åˆ†æå›¾
        self._create_loss_component_figure(reg_results)
        
        # åˆ›å»ºæ•ˆæœå¯¹æ¯”å›¾
        self._create_effect_comparison_figure(data, reg_results)
    
    def _create_pca_analysis_figure(self, data: Dict, pca_results: Dict):
        """åˆ›å»ºPCAåˆ†æè¯¦ç»†å›¾"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Local Geometry Perception: Detailed PCA Analysis\nNeighborhood Structure & Principal Component Analysis', 
                    fontsize=14, fontweight='bold')
        
        positions = data['positions']
        eigenvalues = pca_results['eigenvalues']
        
        # å­å›¾1: ç‰¹å¾å€¼åˆ†å¸ƒ
        eigenval_ratios = eigenvalues[:, 0] / (eigenvalues[:, 2] + 1e-8)
        ax1.hist(eigenval_ratios, bins=15, alpha=0.7, color=self.color_palette[0], edgecolor='black')
        ax1.set_title('PCA Eigenvalue Ratio Distribution')
        ax1.set_xlabel('Primary/Secondary Eigenvalue Ratio')
        ax1.set_ylabel('Frequency')
        ax1.grid(True, alpha=0.3)
        
        # å­å›¾2: 3Dç‚¹äº‘ç€è‰²byä¸»ç‰¹å¾å€¼
        scatter = ax2.scatter(positions[:, 0], positions[:, 1], 
                            c=eigenvalues[:, 0], cmap=self.gradient_cmap, s=50)
        ax2.set_title('Spatial Distribution (Colored by Primary Eigenvalue)')
        ax2.set_xlabel('X')
        ax2.set_ylabel('Y')
        plt.colorbar(scatter, ax=ax2, label='Primary Eigenvalue')
        
        # å­å›¾3: å„å‘å¼‚æ€§ç¨‹åº¦åˆ†æ
        anisotropy_measure = 1 - eigenvalues[:, 2] / (eigenvalues[:, 0] + 1e-8)
        ax3.scatter(eigenval_ratios, anisotropy_measure, 
                   c=self.color_palette[1], alpha=0.7, s=50)
        ax3.set_title('Anisotropy Degree Analysis')
        ax3.set_xlabel('Eigenvalue Ratio')
        ax3.set_ylabel('Anisotropy Measure')
        ax3.grid(True, alpha=0.3)
        
        # å­å›¾4: å±€éƒ¨å‡ ä½•ç±»å‹åˆ†ç±»
        # æ ¹æ®ç‰¹å¾å€¼æ¯”ä¾‹è¿›è¡Œå‡ ä½•ç±»å‹åˆ†ç±»
        planar = eigenval_ratios > 5  # å¹³é¢å‹
        linear = (eigenval_ratios > 2) & (eigenval_ratios <= 5)  # çº¿å‹
        isotropic = eigenval_ratios <= 2  # å„å‘åŒæ€§
        
        labels = ['Planar', 'Linear', 'Isotropic']
        sizes = [np.sum(planar), np.sum(linear), np.sum(isotropic)]
        colors = [self.color_palette[i] for i in range(3)]
        
        wedges, texts, autotexts = ax4.pie(sizes, labels=labels, colors=colors, 
                                          autopct='%1.1f%%', startangle=90)
        ax4.set_title('Local Geometry Type Distribution')
        
        plt.tight_layout()
        
        # ä¿å­˜
        output_path = self.output_dir / 'pca_analysis_detailed.png'
        try:
            plt.savefig(output_path, dpi=self.dpi, bbox_inches='tight', format='png')
            self.logger.info(f"Successfully saved: {output_path}")
        except Exception as e:
            self.logger.error(f"Failed to save PCA analysis: {e}")
        finally:
            plt.close()
        
        self.logger.info(f"âœ… PCAåˆ†æè¯¦ç»†å›¾å·²ä¿å­˜: {output_path}")
    
    def _create_loss_component_figure(self, reg_results: Dict):
        """åˆ›å»ºæŸå¤±åˆ†é‡åˆ†æå›¾"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Mixed Loss Component Analysis\nTraining Dynamics & Component Contributions', 
                    fontsize=14, fontweight='bold')
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å˜åŒ–
        iterations = np.arange(0, 20000, 50)
        
        # ä¸‰ç§æŸå¤±åˆ†é‡çš„æ¨¡æ‹Ÿå˜åŒ–
        alignment_loss = 0.8 * np.exp(-iterations / 5000) + 0.1 + 0.02 * np.sin(iterations / 1000)
        ratio_loss = 0.6 * np.exp(-iterations / 8000) + 0.05 + 0.01 * np.sin(iterations / 1500)
        anisotropy_loss = np.maximum(0, 0.3 * (1 - np.exp(-iterations / 3000))) + 0.005 * np.sin(iterations / 800)
        
        # å­å›¾1: å„æŸå¤±åˆ†é‡éšè®­ç»ƒå˜åŒ–
        ax1.plot(iterations, alignment_loss, label='Axis Alignment Loss', 
                color=self.color_palette[0], linewidth=2)
        ax1.plot(iterations, ratio_loss, label='Scale Ratio Loss', 
                color=self.color_palette[1], linewidth=2)
        ax1.plot(iterations, anisotropy_loss, label='Anisotropy Penalty', 
                color=self.color_palette[2], linewidth=2)
        ax1.set_title('Loss Component Training Curves')
        ax1.set_xlabel('Training Iterations')
        ax1.set_ylabel('Loss Value')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å­å›¾2: æŸå¤±æƒé‡å æ¯”å˜åŒ–
        total_loss = alignment_loss + 0.5 * ratio_loss + 0.1 * anisotropy_loss
        
        # è®¡ç®—ç›¸å¯¹å æ¯”
        align_ratio = alignment_loss / total_loss * 100
        ratio_ratio = (0.5 * ratio_loss) / total_loss * 100
        aniso_ratio = (0.1 * anisotropy_loss) / total_loss * 100
        
        ax2.stackplot(iterations, align_ratio, ratio_ratio, aniso_ratio,
                     labels=['Axis Alignment', 'Scale Ratio', 'Anisotropy'],
                     colors=[self.color_palette[i] for i in range(3)],
                     alpha=0.8)
        ax2.set_title('Loss Weight Proportion Evolution')
        ax2.set_xlabel('Training Iterations')
        ax2.set_ylabel('Weight Proportion (%)')
        ax2.legend(loc='upper right')
        ax2.grid(True, alpha=0.3)
        
        # å­å›¾3: æ¢¯åº¦å¤§å°åˆ†æ
        # æ¨¡æ‹Ÿæ¢¯åº¦å¤§å°
        align_grad = np.gradient(alignment_loss)
        ratio_grad = np.gradient(ratio_loss) * 0.5
        aniso_grad = np.gradient(anisotropy_loss) * 0.1
        
        ax3.semilogy(iterations, np.abs(align_grad), label='Axis Alignment Gradient', 
                    color=self.color_palette[0], alpha=0.8)
        ax3.semilogy(iterations, np.abs(ratio_grad), label='Scale Ratio Gradient', 
                    color=self.color_palette[1], alpha=0.8)
        ax3.semilogy(iterations, np.abs(aniso_grad), label='Anisotropy Gradient', 
                    color=self.color_palette[2], alpha=0.8)
        ax3.set_title('Loss Gradient Magnitude Changes')
        ax3.set_xlabel('Training Iterations')
        ax3.set_ylabel('Gradient Magnitude (log scale)')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # å­å›¾4: æ”¶æ•›é€Ÿåº¦åˆ†æ
        # è®¡ç®—æ”¶æ•›é€Ÿåº¦ï¼ˆæŸå¤±ä¸‹é™ç‡ï¼‰
        window_size = 100
        conv_speed_align = -np.gradient(np.convolve(alignment_loss, np.ones(window_size)/window_size, mode='same'))
        conv_speed_ratio = -np.gradient(np.convolve(ratio_loss, np.ones(window_size)/window_size, mode='same'))
        conv_speed_aniso = -np.gradient(np.convolve(anisotropy_loss, np.ones(window_size)/window_size, mode='same'))
        
        ax4.plot(iterations, conv_speed_align, label='Axis Alignment Convergence', 
                color=self.color_palette[0], linewidth=2)
        ax4.plot(iterations, conv_speed_ratio, label='Scale Ratio Convergence', 
                color=self.color_palette[1], linewidth=2)
        ax4.plot(iterations, conv_speed_aniso, label='Anisotropy Convergence', 
                color=self.color_palette[2], linewidth=2)
        ax4.set_title('Component Convergence Speed')
        ax4.set_xlabel('Training Iterations')
        ax4.set_ylabel('Convergence Rate')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜
        output_path = self.output_dir / 'loss_component_analysis.png'
        try:
            plt.savefig(output_path, dpi=self.dpi, bbox_inches='tight', format='png')
            self.logger.info(f"Successfully saved: {output_path}")
        except Exception as e:
            self.logger.error(f"Failed to save loss component analysis: {e}")
        finally:
            plt.close()
        
        self.logger.info(f"âœ… æŸå¤±åˆ†é‡åˆ†æå›¾å·²ä¿å­˜: {output_path}")
    
    def _create_effect_comparison_figure(self, data: Dict, reg_results: Dict):
        """åˆ›å»ºæ•ˆæœå¯¹æ¯”å›¾"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Regularization Effect Comparison Analysis\nBefore/After Metrics & Performance Evaluation', 
                    fontsize=14, fontweight='bold')
        
        scales_before = data['scales_before']
        scales_after = reg_results['scales_after']
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        ratios_before = scales_before[:, 0] / (scales_before[:, 2] + 1e-8)
        ratios_after = scales_after[:, 0] / (scales_after[:, 2] + 1e-8)
        
        # å­å›¾1: å„å‘å¼‚æ€§æ¯”ä¾‹å¯¹æ¯”æ•£ç‚¹å›¾
        ax1.scatter(ratios_before, ratios_after, c=self.color_palette[0], alpha=0.7, s=50)
        
        # æ·»åŠ å¯¹è§’çº¿ï¼ˆç†æƒ³æƒ…å†µï¼‰
        min_ratio = min(np.min(ratios_before), np.min(ratios_after))
        max_ratio = max(np.max(ratios_before), np.max(ratios_after))
        ax1.plot([min_ratio, max_ratio], [min_ratio, max_ratio], 'k--', alpha=0.5, label='No Change Line')
        
        # æ·»åŠ æ”¹å–„åŒºåŸŸ
        ax1.fill_between([10, max_ratio], 10, max_ratio, alpha=0.2, color='red', label='Over-stretch Region')
        ax1.axhline(y=10, color='green', linestyle='--', alpha=0.7, label='Ideal Upper Limit')
        ax1.axvline(x=10, color='green', linestyle='--', alpha=0.7)
        
        ax1.set_title('Anisotropy Ratio Comparison')
        ax1.set_xlabel('Before Regularization Ratio')
        ax1.set_ylabel('After Regularization Ratio')
        ax1.set_xscale('log')
        ax1.set_yscale('log')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å­å›¾2: æ”¹å–„ç¨‹åº¦ç›´æ–¹å›¾
        improvement = ratios_before - ratios_after
        ax2.hist(improvement, bins=20, alpha=0.7, color=self.color_palette[1], edgecolor='black')
        ax2.axvline(x=0, color='red', linestyle='--', label='No Improvement Line')
        ax2.set_title('Anisotropy Ratio Improvement Distribution')
        ax2.set_xlabel('Improvement Degree (Positive = Better)')
        ax2.set_ylabel('Frequency')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # å­å›¾3: ä½“ç§¯å˜åŒ–åˆ†æ
        volumes_before = np.prod(scales_before, axis=1)
        volumes_after = np.prod(scales_after, axis=1)
        volume_ratio = volumes_after / (volumes_before + 1e-8)
        
        ax3.scatter(volumes_before, volumes_after, c=self.color_palette[2], alpha=0.7, s=50)
        ax3.plot([np.min(volumes_before), np.max(volumes_before)], 
                [np.min(volumes_before), np.max(volumes_before)], 'k--', alpha=0.5)
        ax3.set_title('Gaussian Volume Changes')
        ax3.set_xlabel('Before Regularization Volume')
        ax3.set_ylabel('After Regularization Volume')
        ax3.set_xscale('log')
        ax3.set_yscale('log')
        ax3.grid(True, alpha=0.3)
        
        # å­å›¾4: ç¨³å®šæ€§æŒ‡æ ‡
        # è®¡ç®—å½¢çŠ¶ç¨³å®šæ€§æŒ‡æ ‡
        stability_before = 1 / (1 + ratios_before)  # è¶Šæ¥è¿‘1è¶Šç¨³å®š
        stability_after = 1 / (1 + ratios_after)
        
        n_gaussians = len(stability_before)
        x = np.arange(n_gaussians)
        width = 0.35
        
        bars1 = ax4.bar(x - width/2, stability_before, width, label='Before Regularization', 
                       color=self.color_palette[3], alpha=0.8)
        bars2 = ax4.bar(x + width/2, stability_after, width, label='After Regularization', 
                       color=self.color_palette[4], alpha=0.8)
        
        ax4.set_title('Shape Stability Comparison')
        ax4.set_xlabel('Gaussian Index')
        ax4.set_ylabel('Stability Index')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜
        output_path = self.output_dir / 'effect_comparison_analysis.png'
        try:
            plt.savefig(output_path, dpi=self.dpi, bbox_inches='tight', format='png')
            self.logger.info(f"Successfully saved: {output_path}")
        except Exception as e:
            self.logger.error(f"Failed to save effect comparison analysis: {e}")
        finally:
            plt.close()
        
        self.logger.info(f"âœ… æ•ˆæœå¯¹æ¯”åˆ†æå›¾å·²ä¿å­˜: {output_path}")


def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Principled Mixed Regularization Visualizer")
    parser.add_argument("--output_dir", type=str, default="./regularization_visualization",
                       help="Output directory path")
    parser.add_argument("--model_path", type=str, default=None,
                       help="Training model path (containing point_cloud.ply file)")
    parser.add_argument("--ply_path", type=str, default=None,
                       help="Direct PLY file path")
    parser.add_argument("--use_synthetic", action="store_true",
                       help="Force use synthetic data instead of real data")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = PrincipledMixedRegularizationVisualizer(
        output_dir=args.output_dir,
        model_path=args.model_path
    )
    
    # å¦‚æœæŒ‡å®šäº†PLYè·¯å¾„ï¼Œç›´æ¥åŠ è½½
    if args.ply_path:
        print(f"Loading PLY file: {args.ply_path}")
        real_data = visualizer.load_real_gaussian_data(args.ply_path)
        if real_data:
            print(f"Successfully loaded {real_data['n_gaussians']} Gaussian primitives")
        else:
            print("PLY file loading failed, will use synthetic data")
    
    # ç”Ÿæˆå¯è§†åŒ–
    use_real = not args.use_synthetic
    visualizer.create_mixed_regularization_visualization(use_real_data=use_real)
    
    print("Principled Mixed Regularization visualization completed!")
    print(f"Results saved in: {args.output_dir}")
    print("\nGenerated files:")
    print("  - principled_mixed_regularization.png: Main visualization")
    print("  - pca_analysis_detailed.png: Detailed PCA analysis")
    print("  - loss_component_analysis.png: Loss component analysis")
    print("  - effect_comparison_analysis.png: Effect comparison analysis")
    
    if args.model_path or args.ply_path:
        print("\nUsed real training data for visualization")
    else:
        print("\nUsed synthetic data for demonstration")


if __name__ == "__main__":
    main()